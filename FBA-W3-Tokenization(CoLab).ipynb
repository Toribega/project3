{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AdhSbOSwT70z",
        "-9MGzYEfUcJT",
        "zlZwcrE-8yXf",
        "MuTbHcsV6BLg",
        "fLes_LQ7_TN1",
        "A0o_f1rAGoID",
        "UyMx1Yq-GqIh",
        "sY38lpX8GxfC",
        "k9zsoaPWersA",
        "e3WAEwbakd7f",
        "M4dlH7lCkk4X",
        "HaMDuF_vSs6j",
        "o_IqLzCDSvPt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hJxnNnALVmR"
      },
      "source": [
        "# Concepts of Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXi_05LenIYM"
      },
      "source": [
        "The task of dividing a given text into units called tokens is called tokenization. Although the units of a token vary depending on the situation, you usually define it as meaningful units.\n",
        "\n",
        "In general, the unit of token can be viewed as a sentence in large and a word in small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGAPbS4ZnTAS"
      },
      "source": [
        "Tokenization is generally much better if you don't do it yourself, but rely on packages that have already been implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHOIjaBnZz3"
      },
      "source": [
        "Let's understand what tokenization means through practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhyKqRIS-BeP"
      },
      "source": [
        "Performing word-by-word tokenization is called word tokenization. Shall we practice English and Korean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9LCOBRnid7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFruANiioOeT"
      },
      "source": [
        "#Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y8UCpIaoYIR"
      },
      "source": [
        "Performing word-by-word tokenization is called word tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iciE21O6LaLg"
      },
      "source": [
        "## English : Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZKPNSlTnk4H"
      },
      "source": [
        "When tokenizing in English, we usually use a package called NLTK. NTLK is a package for English natural language processing.\n",
        "In Colab, NLTK is already installed, so you can use it as import nltk right away"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkbmprCkoiSP"
      },
      "source": [
        "NLTK offers a variety of English tokenizers (tools to perform tokenization).\n",
        "**Tokenization results have slightly different rules for each tokenizer. There is no correct answer to which tokenizer to use.**\n",
        "\n",
        "It's up to you to decide which tokenizer to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7dq1AIa4YQw"
      },
      "source": [
        "### **Tokenizer 1 in NLTK =>  word_tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9vsOXPY3Wqe"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofn4FR4V3Ynv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e83d3b7-1343-4163-c4d0-11affeff6eab"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdzieQO1obgJ"
      },
      "source": [
        "If you look at the sentence below, \"Don't\" and \"Jon's\" have an apostrophe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_v2UoFX3DOr"
      },
      "source": [
        "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg_eQnb73i9Z"
      },
      "source": [
        " How can \"Don't\" and \"Jon's\" tokenize with apostrophe in place?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZG-ES853DRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e571ce3c-eb45-409f-90af-a860e9b03b45"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sentence))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnG6YxT3qp_"
      },
      "source": [
        "\"Don't\" is separated into \"Do\" and \"N't\", and \"Zone's\" is separated into \"Zone\" and 's'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmmTR1HsVXe"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCIhWE4mpM4C"
      },
      "source": [
        "### **Tokenizer in NLTK => WordPunctTokenizer**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbwe2_ny3DWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6764cfff-8086-4974-bc34-e6f484452a92"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(sentence))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErJrv2nT3sMP"
      },
      "source": [
        "\"Don't\" is separated into Don and 't',\"Jon's\" is separated into 'Jon' and 's'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFuuDjG_3_i-"
      },
      "source": [
        "**Again, there's no answer to what's better. As a matter of fact, each tokenizer has its own rules, so it's important to choose a tokenizer based on the purpose you want to use.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Umwj-OsUod"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBn2S23BpUY0"
      },
      "source": [
        " ### **Tokenizer 3 in NLTK => TreebankWordTokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zot_67ckA4TZ"
      },
      "source": [
        "Rules of Penn Treebank Tokenizer\n",
        "\n",
        "Rule 1. Keep a word consisting of hyphen as one  \n",
        "Rule 2. Separate words with in apostrophes, such as \"doesn't\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTDxdxLk4-IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5d2998-5aaa-4b78-c16b-d7778574ef5c"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARx4uEMRqg7F"
      },
      "source": [
        "So far, we have performed word tokenization using three tokenizers provided by NLTK from the given sentence string.\n",
        "\n",
        "The results are all different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNZexhH6segP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZCS4energVw"
      },
      "source": [
        "### **Tokenize words based on spacing (preferably No! even if it seems to work out)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hoqLUEtAPh"
      },
      "source": [
        "**In fact, English is pretty good even if you tokenize words based on spacing.**\n",
        "But nevertheless, it's better not to tokenize words based on spacing, so let's understand why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUeLZNUJrGAF"
      },
      "source": [
        "Let's say there's an English sentence like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0a3clHxrH4K"
      },
      "source": [
        "en_text = \"A Dog Run back corner near spare bedrooms!!!!\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EbN949GrMLE"
      },
      "source": [
        "First, let's tokenize with NLTK that we learned earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NocCRtterOG4",
        "outputId": "9a77f1a3-d8b5-4709-85b3-2d03af0c318b"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(en_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms', '!', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnbVGvKAtpd3"
      },
      "source": [
        "It works fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1KPhStArVm8"
      },
      "source": [
        "This time, let's tokenize it by spacing, not NLTK.\n",
        "Python cuts all elements based on spacing and returns them to the form of a list when you type .split() on a given string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BC_cEmbrYqy",
        "outputId": "b63865e4-72b1-4472-951a-424c2b0bc46c"
      },
      "source": [
        "print(en_text.split())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms!!!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vib_14zGrZyp"
      },
      "source": [
        "See? This is the result of word tokenization based on spacing.\n",
        "\n",
        "In fact, in English, if you use a package called NLTK, you can tokenize it more delicately, but just by spacing it out, you can tokenize it almost well. Nevertheless, there is a reason to avoid using spacing as a basis.\n",
        "\n",
        "For example, let's add special characters to an English sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arWjY45-rtc2"
      },
      "source": [
        "en_text = \"A Dog Run back corner near spare bedrooms... bedrooms!!\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkZXiT-zrwyw"
      },
      "source": [
        "Let's tokenize it with NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "232vN1A4rwPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a8b6b8-606a-4841-f2d2-c78226255be6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(en_text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms', '...', 'bedrooms', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcbsBZQPr0Q-"
      },
      "source": [
        "As you can see, even if there are special characters, the bedrooms are separated normally. But what if you tokenize it by spacing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzo9_R4Vr5h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999f2f59-2000-4f97-aa69-96ef2234b1da"
      },
      "source": [
        "print(en_text.split())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms...', 'bedrooms!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aBFM4gAr_eL"
      },
      "source": [
        "\"Bedrooms\" and \"...\" come together,\n",
        "Bedrooms and \"!!!\" are put together, and \"bedrooms!!!\" comes out.\n",
        "\n",
        "In Python's view, they all recognize it as a different word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQU8ZeZosG2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd476de6-9b7b-4267-bcbf-daef48a2679e"
      },
      "source": [
        "if 'bedrooms' == 'bedrooms...':\n",
        "  print('이 둘은 같습니다.')\n",
        "else:\n",
        "  print('이 둘은 다릅니다.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 둘은 다릅니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVhcrmUfsJ22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c1a054-e0ad-4454-f32f-cf3301beb703"
      },
      "source": [
        "'bedrooms...' == 'bedrooms!!'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhhZq9dwtr1X"
      },
      "source": [
        "You can see that NLTK works much more delicately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GbPMPZSoGzF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMrszdyBLQj"
      },
      "source": [
        "## Korean language : Word Tokenization(KoNLPy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJNy3EKWq2qI"
      },
      "source": [
        "### Tokenize words based on spacing (just No!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGJAEiGhrAEE"
      },
      "source": [
        "In fact, in the case of English, the distinction between words is quite clear even if it is tokenized by spacing. But in the case of Korean, tokenization is much more difficult. The reason for this is that the same word is often recognized as a different word when divided into simple spacing units due to particles, etc. in Korean.\n",
        "Tokenizing Korean by spacing is recommended to be rarely used unless there is a clear experimental purpose. Let's understand it through an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xWyJGP-t4Eu",
        "outputId": "de4153e5-3865-47c9-bb3d-b0982dced1a6"
      },
      "source": [
        "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
        "print(kor_text.split())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1eCuBA9ibjuD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM84f6Krt6_C"
      },
      "source": [
        "In the example above, the word \"사과\" appeared four times. All of them have '의', '을', '가', and '랑' attached, so if you don't remove them, the machine will recognize them all as different words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqc89ew5uVFH",
        "outputId": "e6260be7-6647-4f17-9277-a8d81666ea13"
      },
      "source": [
        "'사과' == '사과의'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIBKIli0uafi",
        "outputId": "491e4e50-c702-48bc-ff03-264261399c00"
      },
      "source": [
        "'사과의' == '사과를'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLFh10PPucii",
        "outputId": "1f6aa2ea-0774-442b-c29a-67acc58c6828"
      },
      "source": [
        "'사과를' == '사과가'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-mGZxemueGR",
        "outputId": "5552ec06-759e-4fe3-ad2c-871aeec6412b"
      },
      "source": [
        "'사과가' == '사과랑'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gud9KnwugP-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ca0ljXt8Gg"
      },
      "source": [
        "### Morphological analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGTCavJuCXF"
      },
      "source": [
        "If there is NLTK in English for word tokenization, KoNLPy, a morpheme analyzer package, exists in Korean.\n",
        "KoNLPy is not installed in Colab, so install it separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQMm9UPdBhCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d03e49-3820-4358-8f36-46a03d705711"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W47LzDKLuhjm"
      },
      "source": [
        "Just as NLTK had several tokenizers internally, KoNLPy also has a variety of morpheme analyzers, but the morpheme analyzer called Mecab has to be installed separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j4Q8HvpGrgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1023a179-0a48-4765-f995-a47d09c54503"
      },
      "source": [
        "# Installing Mecab on Colab\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (138/138), 1.72 MiB | 23.42 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2023-09-10 23:33:45--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNJSMABS5Z&Signature=gQDG6UFr95BPSMg8EDKmMcrHgNk%3D&x-amz-security-token=FwoGZXIvYXdzEBkaDFcsqjSwlUjwtwEaPyK%2BASJgXUy0uFjxUYzMhjG7FpGfHmjnt%2Bpo3OIFeteb2Ovi2DMUB9nRU%2B1XSwsIR8QZB2kH5D9CJbKZFbuBfFp3evXAaX4NwQHuUUvQsJ7w%2B5VKrAr%2Bl%2FNhmcgepuUsMzlLad8mXnOSP2vBWr6jrqZTswqgPkSCFZbsZ6503fRwUX5irE4oK8aFxbrLeT4vrdr7gmL5DW%2BACKRP9S6fFcpwPJWJ3lwtzgOFfsHTzVVdtshW2WdbLNa3xE%2F4zz1cg7oo4J%2F5pwYyLV2BD%2BdkiaHAEDhHr%2Fb4nxUgMJuWp26xkQLdcfm0B7EYRYlOp0vVEcD6ZlbhUA%3D%3D&Expires=1694389993 [following]\n",
            "--2023-09-10 23:33:45--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNJSMABS5Z&Signature=gQDG6UFr95BPSMg8EDKmMcrHgNk%3D&x-amz-security-token=FwoGZXIvYXdzEBkaDFcsqjSwlUjwtwEaPyK%2BASJgXUy0uFjxUYzMhjG7FpGfHmjnt%2Bpo3OIFeteb2Ovi2DMUB9nRU%2B1XSwsIR8QZB2kH5D9CJbKZFbuBfFp3evXAaX4NwQHuUUvQsJ7w%2B5VKrAr%2Bl%2FNhmcgepuUsMzlLad8mXnOSP2vBWr6jrqZTswqgPkSCFZbsZ6503fRwUX5irE4oK8aFxbrLeT4vrdr7gmL5DW%2BACKRP9S6fFcpwPJWJ3lwtzgOFfsHTzVVdtshW2WdbLNa3xE%2F4zz1cg7oo4J%2F5pwYyLV2BD%2BdkiaHAEDhHr%2Fb4nxUgMJuWp26xkQLdcfm0B7EYRYlOp0vVEcD6ZlbhUA%3D%3D&Expires=1694389993\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.168.107, 52.217.13.252, 52.217.136.57, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.168.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-09-10 23:33:46 (35.5 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2023-09-10 23:35:04--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNLD7T4A5R&Signature=1RujtQ5geD%2FbJLu93TXz4obNp4A%3D&x-amz-security-token=FwoGZXIvYXdzEBkaDPqKouymnYRkDLa%2BPyK%2BAaTZ3%2BInrFu6Ww60B5IWvgz0to7ILrYv%2B9yaOK3Mr0jkhyRJJpPdC4plSuWxOthV2QJTPQyDRb%2FxuIVIKhYQ%2BOD0zYaid%2FGgkTJd1gNjGhvzK2JYeuUrq2QkzxC4UPpy9HyvUqPKrDzfdqrJ%2Bh5pIzajqrlu5UE78UxrC1B3UgI7yNxWc609R%2FbUWyFdPEwcW9cPJ3alGq4QQHWl%2BUfqkbyRbrweruker3zjWiENK1cTlT5XIFb8MsilBc6UKEMoqKX5pwYyLZdmhiUcMj0NMfuAyI%2FaATBxreQN6eMqASEuUHY2V34sDoJBvDkyDJ5iov%2Bnig%3D%3D&Expires=1694390704 [following]\n",
            "--2023-09-10 23:35:04--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNLD7T4A5R&Signature=1RujtQ5geD%2FbJLu93TXz4obNp4A%3D&x-amz-security-token=FwoGZXIvYXdzEBkaDPqKouymnYRkDLa%2BPyK%2BAaTZ3%2BInrFu6Ww60B5IWvgz0to7ILrYv%2B9yaOK3Mr0jkhyRJJpPdC4plSuWxOthV2QJTPQyDRb%2FxuIVIKhYQ%2BOD0zYaid%2FGgkTJd1gNjGhvzK2JYeuUrq2QkzxC4UPpy9HyvUqPKrDzfdqrJ%2Bh5pIzajqrlu5UE78UxrC1B3UgI7yNxWc609R%2FbUWyFdPEwcW9cPJ3alGq4QQHWl%2BUfqkbyRbrweruker3zjWiENK1cTlT5XIFb8MsilBc6UKEMoqKX5pwYyLZdmhiUcMj0NMfuAyI%2FaATBxreQN6eMqASEuUHY2V34sDoJBvDkyDJ5iov%2Bnig%3D%3D&Expires=1694390704\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 3.5.9.108, 52.217.73.148, 3.5.6.11, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|3.5.9.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  94.5MB/s    in 0.5s    \n",
            "\n",
            "2023-09-10 23:35:05 (94.5 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pxdKtsDK4E"
      },
      "source": [
        "from konlpy.tag import *\n",
        "\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "#mecab = Mecab()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3z0QlzhGEJl"
      },
      "source": [
        "The above morpheme analyzers provide the following functions in common.  \n",
        "nouns : noun extraction    \n",
        "morphs : morpheme extraction  \n",
        "pos : Attaching parts of speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBTatyJgCcdB"
      },
      "source": [
        "### Morphological analyzer Okt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_21rq8q93DgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73f3ab7-00d3-41f8-87a8-cb3bd2d55def"
      },
      "source": [
        "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'못'"
      ],
      "metadata": {
        "id": "YdLMewdtgDYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "망치로 못을 두드리다 => '못' : 명사  \n",
        "나 그 일 못해요. => '못' : 부사"
      ],
      "metadata": {
        "id": "MJ_2GoxUgE7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(\"망치로 못을 두드리다 \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wayWgKQoebUT",
        "outputId": "6a8c361c-168e-4eb5-b978-d65680954cf7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('망치', 'Noun'), ('로', 'Josa'), ('못', 'Noun'), ('을', 'Josa'), ('두드리다', 'Verb')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(\"나 그 일 못해요\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9qyvwfRelKh",
        "outputId": "46dbcd1a-ae98-4d6d-85da-e0487bd863c1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('나', 'Noun'), ('그', 'Noun'), ('일', 'Noun'), ('못', 'VerbPrefix'), ('해요', 'Verb')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIC4M1nCnSQ"
      },
      "source": [
        "### Morphological analyzer 'Kkma'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK7W4wMy3Der",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd7002d-290e-4c02-c6a2-94a4960c67ed"
      },
      "source": [
        "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw5M9NoTDWxv"
      },
      "source": [
        "### Morphological analyzer Komoran"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYr7mO3C3DZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a54e3a-6909-4feb-f534-c54ac98f1ae9"
      },
      "source": [
        "print(komoran.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(komoran.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(komoran.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코', '당신', '연휴', '여행']\n",
            "['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '아', '보', '아요']\n",
            "[('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Morphological analyser: Komoran"
      ],
      "metadata": {
        "id": "4aS5HVOd7s85"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvdqq2Lz3DUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca704aae-4e1f-4041-842f-97a87c2f26e6"
      },
      "source": [
        "print(hannanum.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(hannanum.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(hannanum.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에는', '여행', '을', '가', '아', '보', '아']\n",
            "[('열심히', 'M'), ('코딩', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('당신', 'N'), (',', 'S'), ('연휴', 'N'), ('에는', 'J'), ('여행', 'N'), ('을', 'J'), ('가', 'P'), ('아', 'E'), ('보', 'P'), ('아', 'E')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XrKQ8ZGlVA"
      },
      "source": [
        "### Morphological analyser Mecab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDAAPcmGzw7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "298173cd-e92a-47a8-9589-42597fcd66df"
      },
      "source": [
        "print(mecab.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(mecab.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(mecab.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-eb0e54054e5c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mecab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOXF6YzWKF1J"
      },
      "source": [
        "**Each morpheme analyzer has different performance and results**, so the choice of morpheme analyzer is to determine which morpheme analyzer is most appropriate for the desired application. For example, if you value speed, you can use a 'Mecab'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ykZiIzWiUfr"
      },
      "source": [
        "Note: https://iostream.tistory.com/144 (Compare morpheme analyzer performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJt5ERvooTeG"
      },
      "source": [
        "# Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtEhbe_NzkbN"
      },
      "source": [
        "Sentence tokenization, unlike word tokenization, is used to divide a given text into sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pquF1QYwKLcr"
      },
      "source": [
        "## English : Sentence Tokenization (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMm66t0jiYgQ"
      },
      "source": [
        "Let's say that when you're given a string, you divide it by sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ds6aO6Wi0dK"
      },
      "source": [
        "Yonsei University is a private research university in Seoul, South Korea. Yonsei University is deemed as one of the three most prestigious institutions in the country. It is particularly respected in the studies of medicine and business administration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGHWe5ymikx-"
      },
      "source": [
        "temp = 'Yonsei University is a private research university in Seoul, South Korea. Yonsei University is deemed as one of the three most prestigious institutions in the country. It is particularly respected in the studies of medicine and business administration.'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgROm78jjq39"
      },
      "source": [
        "The string.split ('cut criteria') separates the strings by that criterion and returns them in list form.\n",
        "The code below is the code that cuts the string based on a period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njRK3NrbiyFE",
        "outputId": "faa5831b-dd44-47ad-ae9d-5881f0942e57"
      },
      "source": [
        "temp.split('.')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yonsei University is a private research university in Seoul, South Korea',\n",
              " 'Yonsei University is deemed as one of the three most prestigious institutions in the country',\n",
              " 'It is particularly respected in the studies of medicine and business administration.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic8zCrMlKRI3"
      },
      "source": [
        "Intuitively speaking, what do you think?You might think that you can cut a sentence with a period (.) or a ! criterion, but not necessarily because \"!\" or \"?\" serves as a pretty clear boundary for distinguishing sentences, but not necessarily a period. In other words, a period can appear even if it's not the end of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcqfdjJpKUvp"
      },
      "source": [
        "**IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 ukairia777@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.**  \n",
        "**Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Of0w2EKY0D"
      },
      "source": [
        "There are too many exceptions to separate sentences based on a period.    \n",
        "**NLTK supports sent_tokenize, which performs tokenization of English sentences.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9AKFUkjKNXF"
      },
      "source": [
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjJ4F_DKugH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f73c41c-6044-4829-b49a-9a97e2406492"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKDLuUbKyn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad62b57d-ae6a-46b0-8c3b-a0275dda73ed"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-BWco73K3x9"
      },
      "source": [
        "## Korean language : Sentence Tokenization(KSS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXDQxDBaK5nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe45a81-195b-456b-d5f7-84fe9645f0c4"
      },
      "source": [
        "pip install kss"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-4.5.4.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2023.6.3)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.23.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
            "Building wheels for collected packages: kss, pecab\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-4.5.4-py3-none-any.whl size=54465 sha256=b343115e346f70c0fc842bc6fdf475ba836b3d771d83da36927e7a028e56f766\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7b/ba/e620ef5d96a61cdd83bdee4c2bb4aec8a74de5d72fcbb00e80\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=9962e23eb886183ff802e2caadb92fdb1a5d082e33ad2b3d788ce1e7ea231781\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "Successfully built kss pecab\n",
            "Installing collected packages: emoji, pecab, kss\n",
            "Successfully installed emoji-1.2.0 kss-4.5.4 pecab-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwP2-MfNK9vD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bea2df-dd1e-442d-c943-69cf277311c2"
      },
      "source": [
        "import kss\n",
        "\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    }
  ]
}